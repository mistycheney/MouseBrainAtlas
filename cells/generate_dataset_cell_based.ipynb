{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/__init__.py:1401: UserWarning:  This call to matplotlib.use() has no effect\n",
      "because the backend has already been chosen;\n",
      "matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting environment for AWS compute node\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No vtk\n",
      "File does not exist: /shared/CSHL_data_processed/MD635/MD635_anchor.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD635/MD635_sorted_filenames.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD635/MD635_cropbox.txt\n",
      "File does not exist: /shared/CSHL_data_processed/MD635/MD635_cropbox.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(os.path.join(os.environ['REPO_DIR'], 'utilities'))\n",
    "from utilities2015 import *\n",
    "from metadata import *\n",
    "from data_manager import *\n",
    "from learning_utilities import *\n",
    "from cell_utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>network_model</th>\n",
       "      <th>stain</th>\n",
       "      <th>margins</th>\n",
       "      <th>num_sample_per_class</th>\n",
       "      <th>stacks</th>\n",
       "      <th>cell_features_used</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Inception-BN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>200/500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD585</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Inception-BN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>200/500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD589</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Inception-BN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>200/500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD594</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>NaN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD589</td>\n",
       "      <td>allSizeHist/huMomentsHist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>NaN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD589</td>\n",
       "      <td>largeLargeLinkLenHist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>NaN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD594</td>\n",
       "      <td>largeLargeLinkLenHist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>NaN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD589</td>\n",
       "      <td>largeSizeHist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>NaN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD594</td>\n",
       "      <td>largeSizeHist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>NaN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD589</td>\n",
       "      <td>largeSizeHist/largeLargeLinkLenHist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>NaN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD594</td>\n",
       "      <td>largeSizeHist/largeLargeLinkLenHist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>NaN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD594</td>\n",
       "      <td>largeOrientationHist/largeSizeHist/largeLargeL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>NaN</td>\n",
       "      <td>nissl</td>\n",
       "      <td>500</td>\n",
       "      <td>1000</td>\n",
       "      <td>MD589</td>\n",
       "      <td>largeOrientationHist/largeSizeHist/largeLargeL...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           network_model  stain  margins  num_sample_per_class stacks  \\\n",
       "dataset_id                                                              \n",
       "20          Inception-BN  nissl  200/500                  1000  MD585   \n",
       "21          Inception-BN  nissl  200/500                  1000  MD589   \n",
       "22          Inception-BN  nissl  200/500                  1000  MD594   \n",
       "90                   NaN  nissl      500                  1000  MD589   \n",
       "92                   NaN  nissl      500                  1000  MD589   \n",
       "93                   NaN  nissl      500                  1000  MD594   \n",
       "94                   NaN  nissl      500                  1000  MD589   \n",
       "95                   NaN  nissl      500                  1000  MD594   \n",
       "96                   NaN  nissl      500                  1000  MD589   \n",
       "97                   NaN  nissl      500                  1000  MD594   \n",
       "98                   NaN  nissl      500                  1000  MD594   \n",
       "99                   NaN  nissl      500                  1000  MD589   \n",
       "\n",
       "                                           cell_features_used  \n",
       "dataset_id                                                     \n",
       "20                                                        NaN  \n",
       "21                                                        NaN  \n",
       "22                                                        NaN  \n",
       "90                                  allSizeHist/huMomentsHist  \n",
       "92                                      largeLargeLinkLenHist  \n",
       "93                                      largeLargeLinkLenHist  \n",
       "94                                              largeSizeHist  \n",
       "95                                              largeSizeHist  \n",
       "96                        largeSizeHist/largeLargeLinkLenHist  \n",
       "97                        largeSizeHist/largeLargeLinkLenHist  \n",
       "98          largeOrientationHist/largeSizeHist/largeLargeL...  \n",
       "99          largeOrientationHist/largeSizeHist/largeLargeL...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_id = 90\n",
    "dataset_properties = dataset_settings.loc[dataset_id]\n",
    "\n",
    "num_samples_per_label = dataset_properties['num_sample_per_class']\n",
    "stacks = dataset_properties['stacks'].split('/')\n",
    "\n",
    "cell_features_used = dataset_properties['cell_features_used'].split('/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "structures_to_sample = all_known_structures\n",
    "\n",
    "# negative_labels_to_sample = [s + '_negative' for s in structures_to_sample]\n",
    "\n",
    "margins_to_sample = map(int, str(dataset_properties['margins']).split('/'))\n",
    "\n",
    "surround_positive_labels_to_sample = [convert_to_surround_name(s, margin=m, suffix=surr_l) \n",
    "                             for m in margins_to_sample\n",
    "                             for s in structures_to_sample \n",
    "                             for surr_l in structures_to_sample\n",
    "                             if surr_l != s]\n",
    "\n",
    "surround_noclass_labels_to_sample = [convert_to_surround_name(s, margin=m, suffix='noclass') \n",
    "                             for m in margins_to_sample\n",
    "                             for s in structures_to_sample]\n",
    "\n",
    "labels_to_sample = structures_to_sample + surround_positive_labels_to_sample + surround_noclass_labels_to_sample + ['bg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify region indices by section by label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def identify_predefined_regions_one_section_by_label(stack, sec, labels_to_sample, labeled_contours):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        dict {label: list of region indices}\n",
    "    \"\"\"\n",
    "    \n",
    "    if is_invalid(stack=stack, sec=sec):\n",
    "        sys.stderr.write('Regions on section %d are not identified because the section is invalid.\\n' % sec)\n",
    "        return\n",
    "    \n",
    "    region_contours = load_cell_classifier_data(what='region_contours', stack=stack, sec=sec, ext='bp')\n",
    "    \n",
    "    surround_margins = set([get_margin_from_surround_label(label) for label in labels_to_sample if is_surround_label(label)])\n",
    "    region_labels = label_regions(stack=stack, section=sec, \n",
    "                                  region_contours=region_contours,\n",
    "                                  surround_margins=surround_margins,\n",
    "                                  labeled_contours=labeled_contours)\n",
    "    region_labels = {label: region_indices for label, region_indices in region_labels.iteritems()\n",
    "                     if len(region_indices) > 0}\n",
    "    return region_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for stack in all_annotated_nissl_stacks:\n",
    "# # for stack in ['MD589']:\n",
    "    \n",
    "#     contours_df, _ = DataManager.load_annotation_v3(stack=stack)\n",
    "#     labeled_contours = contours_df[(contours_df['orientation'] == 'sagittal') & (contours_df['downsample'] == 1)].drop_duplicates(subset=['section', 'name', 'side', 'filename', 'downsample', 'creator'])\n",
    "#     labeled_contours = convert_annotation_v3_original_to_aligned_cropped(labeled_contours, stack=stack)\n",
    "\n",
    "#     first_sec, last_sec = metadata_cache['section_limits'][stack]\n",
    "    \n",
    "#     pool = Pool(NUM_CORES/2)\n",
    "#     addresses_by_section_by_label = \\\n",
    "#     pool.map(lambda sec: identify_predefined_regions_one_section_by_label(stack=stack, sec=sec, \n",
    "#                                                              labels_to_sample=labels_to_sample,\n",
    "#                                                              labeled_contours=labeled_contours[labeled_contours['section']==sec]),\n",
    "#              range(first_sec, last_sec+1))\n",
    "#     pool.close()\n",
    "#     pool.join()\n",
    "    \n",
    "#     for sec, region_indices_by_label in zip(range(first_sec, last_sec+1), addresses_by_section_by_label):\n",
    "#         if region_indices_by_label is not None:\n",
    "#             save_hdf_v2(region_indices_by_label, DataManager.get_region_labels_filepath(stack=stack, sec=sec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_predefined_regions_one_section_by_label(stack, sec, labels_to_sample, labeled_contours, num_samples_per_label=None):\n",
    "    \"\"\"\n",
    "    Sample region addresses in a particular section and associate them with different labels (positive, surround, negative, noclass, foreground, background, etc.).\n",
    "    \n",
    "    Args:\n",
    "        region_contours (list of nx2 arrays): list of contour vertices.\n",
    "        labeled_contours (dict of nx2 arrays): {label: contour vertices}\n",
    "        \n",
    "    Returns:\n",
    "        dict of 3-tuple list: {label: list of (stack, section, region_index)}.\n",
    "        If section is invalid, return None.\n",
    "    \"\"\"\n",
    "\n",
    "    addresses = {}\n",
    "    \n",
    "    if is_invalid(stack=stack, sec=sec):\n",
    "        sys.stderr.write('Regions on section %d are not sampled because the section is invalid.\\n' % sec)\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        region_labels = load_hdf_v2(DataManager.get_region_labels_filepath(stack=stack, sec=sec))\n",
    "        region_labels = {label: region_indices for label, region_indices in region_labels.iteritems()\n",
    "                        if label in labels_to_sample}\n",
    "    except Exception as e:\n",
    "        sys.stderr.write('%s\\n' % e)\n",
    "        sys.stderr.write('Cannot load pre-computed region labels for section %d... re-compute\\n' % sec)\n",
    "        region_labels = identify_predefined_regions_one_section_by_label(stack, sec, labels_to_sample, labeled_contours)\n",
    "\n",
    "    for label, region_indices in region_labels.iteritems():\n",
    "        if num_samples_per_label is None:\n",
    "            addresses[label] = [(stack, sec, ridx) for ridx in region_indices]\n",
    "        else:\n",
    "            sampled_region_indices = np.random.choice(region_indices, min(num_samples_per_label, len(region_indices)), replace=False)\n",
    "            addresses[label] = [(stack, sec, ridx) for ridx in sampled_region_indices]\n",
    "\n",
    "    return addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_cell_based_features_one_section_(stack, section, region_indices, cell_features_used):\n",
    "    \"\"\"\n",
    "    Load pre-computed cell-based features for a list of regions on a particular section.\n",
    "    \"\"\"\n",
    "    \n",
    "    region_features_all_regions = load_cell_classifier_data(what='region_features', stack=stack, sec=section, ext='hdf')\n",
    "    # Loading hdf ~ 2 seconds.\n",
    "    \n",
    "    all_features_normalized = []\n",
    "    for feature_name in cell_features_used:\n",
    "        feats = np.asarray([region_features_all_regions[region_ind][feature_name] for region_ind in region_indices])\n",
    "        \n",
    "        # huMomentsHist feats: n_regions x 7 x n_bins\n",
    "        # other feats: n_regions x n_bins\n",
    "        \n",
    "        feats_normalized = feats/feats.sum(axis=-1)[...,None].astype(np.float)\n",
    "        if feature_name == 'huMomentsHist':\n",
    "            feats_normalized = np.reshape(feats_normalized, (feats_normalized.shape[0], -1))\n",
    "        all_features_normalized.append(feats_normalized)\n",
    "\n",
    "    features = np.hstack(all_features_normalized)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_dataset_cell_based(num_samples_per_label, stacks, labels_to_sample, cell_features_used):\n",
    "    \"\"\"\n",
    "    Generate dataset.\n",
    "    - Extract addresses\n",
    "    - Map addresses to features\n",
    "    - Remove None features\n",
    "    \n",
    "    Returns:\n",
    "        features, addresseslab\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sample addresses\n",
    "    \n",
    "    addresses = defaultdict(list)\n",
    "    addresses_by_section_by_label = []\n",
    "    \n",
    "#     t = time.time()\n",
    "    \n",
    "    for stack in stacks:\n",
    "        \n",
    "        first_sec, last_sec = metadata_cache['section_limits'][stack]\n",
    "    \n",
    "        t1 = time.time()\n",
    "\n",
    "        contours_df, _ = DataManager.load_annotation_v3(stack=stack)\n",
    "        labeled_contours = contours_df[(contours_df['orientation'] == 'sagittal') & (contours_df['downsample'] == 1)].drop_duplicates(subset=['section', 'name', 'side', 'filename', 'downsample', 'creator'])\n",
    "        labeled_contours = convert_annotation_v3_original_to_aligned_cropped(labeled_contours, stack=stack)\n",
    "\n",
    "        sys.stderr.write('Load annotation. Time: %.2f seconds.\\n' % (time.time() - t1))\n",
    "\n",
    "        t1 = time.time()\n",
    "        \n",
    "        # Sample addresses from each section\n",
    "\n",
    "        # If region labels are already computed, sequential loading is much faster (3s) than parallel loading (50s) due to disk read contention.\n",
    "        addresses_by_section_by_label_curr_stack = [\n",
    "            sample_predefined_regions_one_section_by_label(stack=stack, sec=sec, \n",
    "                                                                 labels_to_sample=labels_to_sample,\n",
    "                                                                 labeled_contours=labeled_contours[labeled_contours['section']==sec],\n",
    "                                                                 num_samples_per_label=30)\n",
    "            for sec in range(first_sec, last_sec+1)]\n",
    "\n",
    "#         pool = Pool(NUM_CORES/2)\n",
    "#         addresses_by_section_by_label_curr_stack = \\\n",
    "#         pool.map(lambda sec: sample_predefined_regions_one_section_by_label(stack=stack, sec=sec, \n",
    "#                                                                  labels_to_sample=labels_to_sample,\n",
    "#                                                                  labeled_contours=labeled_contours[labeled_contours['section']==sec],\n",
    "#                                                                  num_samples_per_label=30),\n",
    "#                  range(first_sec, last_sec+1))\n",
    "#         pool.close()\n",
    "#         pool.join()\n",
    "                \n",
    "        addresses_by_section_by_label += addresses_by_section_by_label_curr_stack\n",
    "\n",
    "        sys.stderr.write('Sample addresses (stack %s): %.2s seconds.\\n' % (stack, time.time() - t1))\n",
    "        \n",
    "    # Aggregate addresses sampled form each section\n",
    "    \n",
    "    t = time.time()\n",
    "    \n",
    "    addresses_by_label = defaultdict(list)\n",
    "    for addrs_by_label in addresses_by_section_by_label:\n",
    "        if addrs_by_label is not None: # handle cases of invalid sections for which sampling function returns None\n",
    "            for label, addrs in addrs_by_label.iteritems():\n",
    "                addresses_by_label[label] += addrs\n",
    "    addresses_by_label.default_factory = None\n",
    "    \n",
    "    if num_samples_per_label is not None:\n",
    "        import random\n",
    "        addresses_by_label = {label: random.sample(addrs, min(num_samples_per_label/len(stacks), len(addrs))) \n",
    "                              for label, addrs in addresses_by_label.iteritems()}\n",
    "\n",
    "    # Remove unwanted labels\n",
    "    addresses_by_label = {label: addrs for label, addrs in addresses_by_label.iteritems() if label in labels_to_sample}\n",
    "    \n",
    "    sys.stderr.write('Aggregate addresses: %.2f seconds\\n' % (time.time() - t))    \n",
    "    \n",
    "    # Map addresses to features\n",
    "    \n",
    "    t = time.time()\n",
    "    \n",
    "    load_cell_based_features_given_address_list = lambda addrs: smart_map(addrs, keyfunc=lambda (st, se, ri): (st, se),\\\n",
    "                        func=lambda (st, se), gr: \\\n",
    "                        load_cell_based_features_one_section_(stack=st, section=se, \n",
    "                                                              region_indices=[ri for _,_,ri in gr], cell_features_used=cell_features_used))\n",
    "    features_by_label = apply_function_to_dict(load_cell_based_features_given_address_list, addresses_by_label)\n",
    "    features_by_label = apply_function_to_dict(np.asarray, features_by_label)\n",
    "    \n",
    "    sys.stderr.write('Map addresses to features: %.2f seconds\\n' % (time.time() - t))\n",
    "    \n",
    "    # Remove features that are None are contain nan values.\n",
    "\n",
    "    for name in features_by_label.keys():\n",
    "        valid = [(ftr, addr) for ftr, addr in zip(features_by_label[name], addresses_by_label[name])\n",
    "                    if ftr is not None and not np.any(np.isnan(ftr))]\n",
    "        res = zip(*valid)\n",
    "        features_by_label[name] = np.array(res[0])\n",
    "        addresses_by_label[name] = res[1]\n",
    "    \n",
    "    return features_by_label, addresses_by_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'No object named structures in the file'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Annotation has no structures.\n",
      "Load annotation. Time: 1.38 seconds.\n",
      "Regions on section 118 are not sampled because the section is invalid.\n",
      "Regions on section 221 are not sampled because the section is invalid.\n",
      "Regions on section 300 are not sampled because the section is invalid.\n",
      "Regions on section 324 are not sampled because the section is invalid.\n",
      "Sample addresses (stack MD589): 1. seconds.\n",
      "Aggregate addresses: 0.03 seconds\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in divide\n",
      "  app.launch_new_instance()\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in divide\n",
      "  app.launch_new_instance()\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in divide\n",
      "  app.launch_new_instance()\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in divide\n",
      "  app.launch_new_instance()\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in divide\n",
      "  app.launch_new_instance()\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in divide\n",
      "  app.launch_new_instance()\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in divide\n",
      "  app.launch_new_instance()\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in divide\n",
      "  app.launch_new_instance()\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in divide\n",
      "  app.launch_new_instance()\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in divide\n",
      "  app.launch_new_instance()\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in divide\n",
      "  app.launch_new_instance()\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in divide\n",
      "  app.launch_new_instance()\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in divide\n",
      "  app.launch_new_instance()\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in divide\n",
      "  app.launch_new_instance()\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in divide\n",
      "  app.launch_new_instance()\n",
      "Map addresses to features: 56.48 seconds\n"
     ]
    }
   ],
   "source": [
    "features, addresses = generate_dataset_cell_based(num_samples_per_label=num_samples_per_label, \n",
    "                                                  stacks=stacks,\n",
    "                                                  labels_to_sample=labels_to_sample,\n",
    "                                                 cell_features_used=cell_features_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10N 10\n",
      "10N_surround_500_12N 952\n",
      "10N_surround_500_noclass 998\n",
      "12N 1000\n",
      "12N_surround_500_10N 10\n",
      "12N_surround_500_AP 11\n",
      "12N_surround_500_noclass 975\n",
      "3N 692\n",
      "3N_surround_500_noclass 692\n",
      "4N_surround_500_3N 91\n",
      "4N_surround_500_noclass 340\n",
      "5N 1000\n",
      "5N_surround_500_7n 13\n",
      "5N_surround_500_noclass 995\n",
      "6N 23\n",
      "6N_surround_500_7n 8\n",
      "6N_surround_500_noclass 149\n",
      "7N 1000\n",
      "7N_surround_500_noclass 998\n",
      "7n 376\n",
      "7n_surround_500_5N 2\n",
      "7n_surround_500_6N 5\n",
      "7n_surround_500_noclass 956\n",
      "AP 335\n",
      "AP_surround_500_12N 18\n",
      "AP_surround_500_noclass 416\n",
      "Amb 205\n",
      "Amb_surround_500_7N 3\n",
      "Amb_surround_500_noclass 390\n",
      "DC 1000\n",
      "DC_surround_500_VCA 898\n",
      "DC_surround_500_VCP 995\n",
      "DC_surround_500_noclass 934\n",
      "IC 1000\n",
      "IC_surround_500_SC 1000\n",
      "IC_surround_500_noclass 932\n",
      "LC 713\n",
      "LC_surround_500_noclass 717\n",
      "LRt 1000\n",
      "LRt_surround_500_Sp5C 130\n",
      "LRt_surround_500_noclass 999\n",
      "PBG 259\n",
      "PBG_surround_500_SNR 17\n",
      "PBG_surround_500_VLL 98\n",
      "PBG_surround_500_noclass 777\n",
      "Pn 923\n",
      "Pn_surround_500_RtTg 176\n",
      "Pn_surround_500_VLL 24\n",
      "Pn_surround_500_noclass 892\n",
      "RMC 963\n",
      "RMC_surround_500_noclass 964\n",
      "RtTg 390\n",
      "RtTg_surround_500_Pn 223\n",
      "RtTg_surround_500_Tz 49\n",
      "RtTg_surround_500_noclass 394\n",
      "SC 999\n",
      "SC_surround_500_IC 1000\n",
      "SC_surround_500_noclass 982\n",
      "SNC 571\n",
      "SNC_surround_500_SNR 986\n",
      "SNC_surround_500_noclass 989\n",
      "SNR 934\n",
      "SNR_surround_500_PBG 3\n",
      "SNR_surround_500_SNC 571\n",
      "SNR_surround_500_VLL 26\n",
      "SNR_surround_500_noclass 940\n",
      "Sp5C 999\n",
      "Sp5C_surround_500_LRt 18\n",
      "Sp5C_surround_500_Sp5I 630\n",
      "Sp5C_surround_500_noclass 997\n",
      "Sp5I 1000\n",
      "Sp5I_surround_500_Sp5C 630\n",
      "Sp5I_surround_500_Sp5O 1000\n",
      "Sp5I_surround_500_noclass 938\n",
      "Sp5O 991\n",
      "Sp5O_surround_500_Sp5I 1000\n",
      "Sp5O_surround_500_noclass 904\n",
      "Tz 1000\n",
      "Tz_surround_500_RtTg 100\n",
      "Tz_surround_500_noclass 977\n",
      "VCA 1000\n",
      "VCA_surround_500_DC 878\n",
      "VCA_surround_500_VCP 999\n",
      "VCA_surround_500_noclass 926\n",
      "VCP 991\n",
      "VCP_surround_500_DC 1000\n",
      "VCP_surround_500_VCA 972\n",
      "VCP_surround_500_noclass 885\n",
      "VLL 991\n",
      "VLL_surround_500_PBG 21\n",
      "VLL_surround_500_SNR 15\n",
      "VLL_surround_500_noclass 937\n",
      "bg 720\n"
     ]
    }
   ],
   "source": [
    "for l, v in sorted(addresses.items()):\n",
    "    print l, len(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws s3 cp /shared/CSHL_cells_v2/classifiers/datasets/dataset_93/patch_features.hdf s3://mousebrainatlas-data/CSHL_cells_v2/classifiers/datasets/dataset_93/patch_features.hdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Child returned 0\n",
      "0.52 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws s3 cp /shared/CSHL_cells_v2/classifiers/datasets/dataset_93/patch_addresses.pkl s3://mousebrainatlas-data/CSHL_cells_v2/classifiers/datasets/dataset_93/patch_addresses.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Child returned 0\n",
      "0.52 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Save features\n",
    "features_fp = os.path.join(CELL_FEATURES_CLF_ROOTDIR, 'datasets', 'dataset_%d' % dataset_id, 'patch_features.hdf')\n",
    "create_parent_dir_if_not_exists(features_fp)\n",
    "save_hdf_v2(features, features_fp)\n",
    "upload_to_s3(features_fp)\n",
    "# train_feat_dir = create_if_not_exists(os.path.join(CLF_ROOTDIR, 'datasets', 'dataset_%d' % dataset, 'patch_features'))\n",
    "# for label, feats in training_features.iteritems():\n",
    "#     bp.pack_ndarray_file(feats, os.path.join(train_feat_dir, label + '.bp'))\n",
    "\n",
    "# Save addresses\n",
    "addresses_fp = os.path.join(CELL_FEATURES_CLF_ROOTDIR, 'datasets', 'dataset_%d' % dataset_id, 'patch_addresses.pkl')\n",
    "save_pickle(addresses, addresses_fp)\n",
    "upload_to_s3(addresses_fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
